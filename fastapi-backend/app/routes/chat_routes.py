"""
Chat API Routes for RAG Chatbot
"""

from fastapi import APIRouter, HTTPException, Request
from fastapi.responses import StreamingResponse
import google.generativeai as genai
import asyncio
import json
import logging
from typing import AsyncGenerator

from app.config.settings import settings
from app.models.chat import ChatRequest, ChatResponse, ContextSource, ErrorResponse, StreamChunk
from app.services.rag_service import retrieve_context, search_all_collections

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/chat", tags=["Chat"])

# Configure Gemini
genai.configure(api_key=settings.GEMINI_API_KEY)


def build_prompt(user_message: str, context: str, conversation_history: list = None) -> str:
    """
    Build prompt for Gemini with RAG context and conversation history.
    """
    system_prompt = """B·∫°n l√† tr·ª£ l√Ω AI th√¥ng minh cho c·ª≠a h√†ng th·ªùi trang Veloura. 
Nhi·ªám v·ª• c·ªßa b·∫°n l√† gi√∫p kh√°ch h√†ng t√¨m s·∫£n ph·∫©m, tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ th·ªùi trang, v√† t∆∞ v·∫•n mua s·∫Øm.

H∆Ø·ªöNG D·∫™N:
- Lu√¥n th√¢n thi·ªán, nhi·ªát t√¨nh v√† chuy√™n nghi·ªáp
- S·ª≠ d·ª•ng th√¥ng tin t·ª´ CONTEXT b√™n d∆∞·ªõi ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c
- **QUAN TR·ªåNG**: Khi gi·ªõi thi·ªáu s·∫£n ph·∫©m, PH·∫¢I li·ªát k√™ T·∫§T C·∫¢ s·∫£n ph·∫©m c√≥ trong CONTEXT, KH√îNG ƒë∆∞·ª£c b·ªè s√≥t
- Format m·ªói s·∫£n ph·∫©m th√†nh m·ª•c c√≥ s·ªë th·ª© t·ª± r√µ r√†ng
- M·ªói s·∫£n ph·∫©m PH·∫¢I bao g·ªìm: t√™n, gi√°, v√† link (n·∫øu c√≥ trong context)
- Format link nh∆∞ sau: üîó Xem chi ti·∫øt: [link]
- N·∫øu h·ªèi v·ªÅ gi√°, n√™u r√µ gi√° b·∫±ng VNƒê (v√≠ d·ª•: 500,000‚Ç´)
- N·∫øu kh√¥ng c√≥ th√¥ng tin trong CONTEXT, h√£y n√≥i r√µ v√† ƒë∆∞a ra g·ª£i √Ω chung
- Tr·∫£ l·ªùi ng·∫Øn g·ªçn, s√∫c t√≠ch, d·ªÖ hi·ªÉu
- Kh√¥ng b·ªãa ƒë·∫∑t th√¥ng tin kh√¥ng c√≥ trong CONTEXT

V√ç D·ª§ FORMAT RESPONSE:
D·∫°, em t√¨m th·∫•y c√°c s·∫£n ph·∫©m √°o thun nam sau:

1. **√Åo Thun Basic Nam**
   - Gi√°: 250,000‚Ç´
   - üîó Xem chi ti·∫øt: http://localhost:5173/product/123

2. **√Åo Thun Polo Nam**
   - Gi√°: 350,000‚Ç´
   - üîó Xem chi ti·∫øt: http://localhost:5173/product/456

3. **√Åo Thun D·ªát Kim**
   - Gi√°: 299,000‚Ç´
   - üîó Xem chi ti·∫øt: http://localhost:5173/product/789

"""
    
    # Add conversation history if available
    if conversation_history:
        history_text = "\n\nL·ªäCH S·ª¨ H·ªòI THO·∫†I:\n"
        for msg in conversation_history[-5:]:  # Last 5 messages
            role = "Ng∆∞·ªùi d√πng" if msg.role == "user" else "Tr·ª£ l√Ω"
            history_text += f"{role}: {msg.content}\n"
        system_prompt += history_text
    
    # Add context
    prompt = f"""{system_prompt}

CONTEXT (Th√¥ng tin t·ª´ c∆° s·ªü d·ªØ li·ªáu):
{context}

---

C√¢u h·ªèi c·ªßa kh√°ch h√†ng: {user_message}

Tr·∫£ l·ªùi (b·∫±ng ti·∫øng Vi·ªát):"""
    
    return prompt


@router.post("/", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    Chat endpoint with RAG (Retrieval-Augmented Generation).
    
    - Nh·∫≠n tin nh·∫Øn t·ª´ ng∆∞·ªùi d√πng
    - T√¨m ki·∫øm th√¥ng tin li√™n quan trong database (RAG)
    - G·ªçi Gemini API ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi
    - Tr·∫£ v·ªÅ response k√®m sources
    """
    try:
        logger.info(f"Chat request: '{request.message[:50]}...'")
        
        # Step 1: Retrieve context from vector search
        context = ""
        sources = []
        
        if request.include_context:
            # Search all collections with more results
            search_results = await search_all_collections(request.message, top_k_per_collection=5)
            
            # Extract sources
            for collection_name, results in search_results.items():
                for doc in results:
                    sources.append(ContextSource(
                        collection=collection_name,
                        id=doc.get("_id", ""),
                        title=doc.get("name") or doc.get("title", ""),
                        score=doc.get("score", 0.0)
                    ))
            
            # Format context
            context = await retrieve_context(request.message, top_k=5)
        
        # Step 2: Build prompt
        prompt = build_prompt(
            user_message=request.message,
            context=context,
            conversation_history=request.conversation_history
        )
        
        # Step 3: Call Gemini API
        model = genai.GenerativeModel(settings.GEMINI_MODEL)
        
        response = await asyncio.to_thread(
            model.generate_content,
            prompt
        )
        
        assistant_message = response.text
        
        logger.info(f"Generated response ({len(assistant_message)} chars)")
        
        return ChatResponse(
            success=True,
            message=assistant_message,
            sources=sources[:5]  # Return top 5 sources
        )
        
    except Exception as e:
        logger.error(f"Chat error: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to generate response: {str(e)}"
        )


async def generate_stream(prompt: str) -> AsyncGenerator[str, None]:
    """
    Generate streaming response from Gemini.
    """
    try:
        model = genai.GenerativeModel(settings.GEMINI_MODEL)
        
        response = await asyncio.to_thread(
            model.generate_content,
            prompt,
            stream=True
        )
        
        for chunk in response:
            if chunk.text:
                # Send as Server-Sent Events format
                data = StreamChunk(content=chunk.text, done=False)
                yield f"data: {data.model_dump_json()}\n\n"
                await asyncio.sleep(0.01)  # Small delay for smooth streaming
        
        # Send final chunk
        final_chunk = StreamChunk(content="", done=True)
        yield f"data: {final_chunk.model_dump_json()}\n\n"
        
    except Exception as e:
        logger.error(f"Streaming error: {str(e)}")
        error_chunk = StreamChunk(content=f"Error: {str(e)}", done=True)
        yield f"data: {error_chunk.model_dump_json()}\n\n"


@router.post("/stream")
async def chat_stream(request: ChatRequest):
    """
    Streaming chat endpoint with Server-Sent Events (SSE).
    
    - Tr·∫£ v·ªÅ response theo t·ª´ng chunk
    - T·ªët h∆°n cho UX (ng∆∞·ªùi d√πng th·∫•y response t·ª´ t·ª´)
    - S·ª≠ d·ª•ng EventSource ·ªü frontend ƒë·ªÉ nh·∫≠n
    """
    try:
        logger.info(f"Stream request: '{request.message[:50]}...'")
        
        # Retrieve context
        context = ""
        if request.include_context:
            context = await retrieve_context(request.message, top_k=3)
        
        # Build prompt
        prompt = build_prompt(
            user_message=request.message,
            context=context,
            conversation_history=request.conversation_history
        )
        
        # Return streaming response
        return StreamingResponse(
            generate_stream(prompt),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"  # Disable nginx buffering
            }
        )
        
    except Exception as e:
        logger.error(f"Stream error: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Streaming failed: {str(e)}"
        )


@router.get("/health")
async def health_check():
    """
    Health check endpoint for chat service.
    """
    try:
        # Test Gemini API
        model = genai.GenerativeModel(settings.GEMINI_MODEL)
        response = await asyncio.to_thread(
            model.generate_content,
            "Hello"
        )
        
        return {
            "success": True,
            "status": "healthy",
            "gemini_api": "connected",
            "model": settings.GEMINI_MODEL
        }
    except Exception as e:
        return {
            "success": False,
            "status": "unhealthy",
            "error": str(e)
        }
